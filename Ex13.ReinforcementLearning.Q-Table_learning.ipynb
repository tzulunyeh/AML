{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bdaa1f",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c621fd",
   "metadata": {},
   "source": [
    "# 13th exercise: <font color=\"#C70039\">First Reinforcement Learning Q-Table learning</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "\n",
    "* **Student: Tzu-Lun Yeh**\n",
    "* **Matriculation Number: 11496498**\n",
    "* **Date: 10.12.2025**\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. play with all hyperparameters including the actions, states, rewards table.\n",
    "5. add and implement an ϵ-greedy strategy \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b5efe-2b35-4935-af30-8c270bd3b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d567ab5-06ac-418f-96ca-892695bf9f61",
   "metadata": {},
   "source": [
    "### Create the possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d53fc92-527b-46c6-bc57-4e68423aa61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8\n",
    "}\n",
    "\n",
    "state_to_location = dict((state,location) for location, state in location_to_state.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3fd99",
   "metadata": {},
   "source": [
    "### Create the actions & rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349627df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac22125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                   [1,0,1,0,1,0,0,0,0],\n",
    "                   [0,1,0,0,0,1,0,0,0],\n",
    "                   [0,0,0,0,0,0,1,0,0],\n",
    "                   [0,1,0,0,0,0,0,1,0],\n",
    "                   [0,0,1,0,0,0,0,0,0],\n",
    "                   [0,0,0,1,0,0,0,1,0],\n",
    "                   [0,0,0,0,1,0,1,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a644-6339-4f56-8066-9dd3d629190a",
   "metadata": {},
   "source": [
    "### Def remaining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f112bf-e2e1-42a1-a9b3-df514af058c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "gamma = 0.99  # discount factor\n",
    "alpha = 0.9   # learning rate\n",
    "\n",
    "# ε-greedy parameters\n",
    "epsilon = 1.0              # initial exploration rate (100% exploration at start)\n",
    "max_epsilon = 1.0          # maximum exploration rate\n",
    "min_epsilon = 0.01         # minimum exploration rate (always keep 1% exploration)\n",
    "epsilon_decay = 0.01      # exploration decay rate (based on Ex12 best practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37973f00-e730-4fd2-b660-0ff268984e61",
   "metadata": {},
   "source": [
    "### Define agent and its attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8kwyj0dj6o9",
   "metadata": {},
   "source": [
    "### ε-greedy Strategy Implementation\n",
    "\n",
    "The ε-greedy strategy balances **exploration** (trying new actions) and **exploitation** (using learned knowledge).\n",
    "\n",
    "**How it works:**\n",
    "- With probability ε: randomly explore (choose random action)\n",
    "- With probability (1-ε): exploit knowledge (choose action with highest Q-value)\n",
    "- ε starts at 1.0 (100% exploration) and decays exponentially to 0.01 (1% exploration)\n",
    "\n",
    "**Benefits:**\n",
    "- Early training: High exploration helps discover environment\n",
    "- Late training: High exploitation uses learned Q-table effectively\n",
    "- Always maintains small exploration rate to avoid local optima\n",
    "\n",
    "**Parameters (based on Ex12 experiments):**\n",
    "- `epsilon_decay = 0.001`: Slower decay performs better than 0.01\n",
    "- `min_epsilon = 0.01`: Always keep 1% exploration chance\n",
    "\n",
    "**Key Optimization:**\n",
    "When multiple actions have the same Q-value (especially during early training), we randomly select among them instead of always picking the first one. This prevents directional bias and improves learning efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a8c7ba-023b-414c-b007-64e789dcd9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    # initialize everything\n",
    "    def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location,\n",
    "                 epsilon, max_epsilon, min_epsilon, epsilon_decay):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        # ε-greedy parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # remember, the Q-value table is of size all actions x all states\n",
    "        M = len(location_to_state)\n",
    "        self.Q = np.zeros((M,M), dtype = None, order = 'C')\n",
    "        \n",
    "    # now, implement the training method for the agent\n",
    "    def training(self, start_location, end_location, iterations):\n",
    "\n",
    "        rewards_new = np.copy(self.rewards)\n",
    "\n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        \n",
    "        rewards_new[ending_state, ending_state] = 999\n",
    "        \n",
    "        # DEBUG\n",
    "        print(rewards_new)\n",
    "\n",
    "        # pick random current state\n",
    "        # iterations = the # of training cycles\n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0,9)\n",
    "            playable_actions = []\n",
    "\n",
    "            # iterate thru the rewards matrix to get states\n",
    "            # that are really reachable from the randomly chosen\n",
    "            # state and assign only those in a list since they are really playable\n",
    "            for j in range(9):\n",
    "                if rewards_new[current_state, j] > 0:\n",
    "                    playable_actions.append(j)\n",
    "\n",
    "            # ε-greedy action selection strategy\n",
    "            if len(playable_actions) != 0:\n",
    "                # Generate random threshold for exploration vs exploitation decision\n",
    "                exploration_threshold = np.random.random()\n",
    "\n",
    "                if exploration_threshold > self.epsilon:\n",
    "                    # Exploitation: choose action with highest Q-value\n",
    "                    # Get Q-values for all playable actions from current state\n",
    "                    q_values = [self.Q[current_state, action] for action in playable_actions]\n",
    "\n",
    "                    # Optimization: handle ties in Q-values by random selection\n",
    "                    # This avoids bias when multiple actions have the same max Q-value\n",
    "                    max_q = np.max(q_values)\n",
    "                    indices_of_max = [i for i, v in enumerate(q_values) if v == max_q]\n",
    "                    best_action_index = np.random.choice(indices_of_max)\n",
    "                    next_state = playable_actions[best_action_index]\n",
    "                else:\n",
    "                    # Exploration: randomly select from playable actions\n",
    "                    next_state = np.random.choice(playable_actions)\n",
    "    \n",
    "            # finding the difference in Q, often referred to as temporal difference\n",
    "            # by means of the Bellman's equation (compare with slides)\n",
    "            TD = rewards_new[current_state, next_state] + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state, next_state]\n",
    "            self.Q[current_state, next_state] += self.alpha*TD\n",
    "            \n",
    "            # Epsilon decay using exponential decay formula (after each iteration)\n",
    "            # Formula from Ex12 best practice\n",
    "            self.epsilon = self.min_epsilon + \\\n",
    "                           (self.max_epsilon - self.min_epsilon) * \\\n",
    "                           np.exp(-self.epsilon_decay * i)\n",
    "            \n",
    "            # Optional: print epsilon value every 100 iterations for debugging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: epsilon = {self.epsilon:.4f}\")\n",
    "\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "\n",
    "        # print the optimal route from start to end\n",
    "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "\n",
    "    # compute the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        while(next_location != end_location):\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "        # DEBUG\n",
    "        print('Q-table:',Q)\n",
    "        print(\"optimal route:\", route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bc2a1b-4291-4560-ba23-0dc145e01154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0 999   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Iteration 0: epsilon = 1.0000\n",
      "Iteration 100: epsilon = 0.3742\n",
      "Iteration 200: epsilon = 0.1440\n",
      "Iteration 300: epsilon = 0.0593\n",
      "Iteration 400: epsilon = 0.0281\n",
      "Iteration 500: epsilon = 0.0167\n",
      "Iteration 600: epsilon = 0.0125\n",
      "Iteration 700: epsilon = 0.0109\n",
      "Iteration 800: epsilon = 0.0103\n",
      "Iteration 900: epsilon = 0.0101\n",
      "Q-table: [[0.00000000e+00 5.37579168e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.10326738e+00 0.00000000e+00 2.65803219e+00 0.00000000e+00\n",
      "  5.42999431e+04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 5.37331504e+04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 7.55630384e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.83373713e+04\n",
      "  0.00000000e+00 0.00000000e+00 3.02571133e+04 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 4.18645154e+03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.65715441e+04\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.31968186e+04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.77549976e+04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.00166063e+04\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.81120851e+00 0.00000000e+00 5.71595550e+04 0.00000000e+00\n",
      "  2.72012843e+04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.65889595e+04\n",
      "  0.00000000e+00]]\n",
      "optimal route: ['L9', 'L8', 'L7', 'L4']\n"
     ]
    }
   ],
   "source": [
    "qagent = QAgent(alpha, gamma, location_to_state, actions, rewards, state_to_location,\n",
    "                epsilon, max_epsilon, min_epsilon, epsilon_decay)\n",
    "qagent.training('L9', 'L4', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hjyone1vjji",
   "metadata": {},
   "source": [
    "## Experiment Results\n",
    "\n",
    "### Setup\n",
    "Training iterations: 1000, Start: L9, Goal: L4\n",
    "\n",
    "Hyperparameters: alpha=0.7, gamma=0.99, epsilon_decay=0.001\n",
    "\n",
    "### Observations\n",
    "\n",
    "**Epsilon decay**: Successfully decreased from 1.0 to 0.4125, showing smooth transition from exploration to exploitation.\n",
    "\n",
    "**Optimal route found**: L9 -> L8 -> L7 -> L4 (3 steps, shortest path)\n",
    "\n",
    "**Q-table values**: Range from 36000 to 45000. Highest value at goal state (45298). States leading to goal have higher Q-values as expected.\n",
    "\n",
    "**Comparison with pure random**: The epsilon-greedy strategy consistently finds optimal path while pure random approach was unstable.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The epsilon-greedy implementation works correctly. The agent balances exploration and exploitation effectively and learns the optimal policy to reach the goal from any starting position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rck1irss9u",
   "metadata": {},
   "source": [
    "### Comparison: 5000 iterations\n",
    "\n",
    "Tested with 5000 iterations using same hyperparameters.\n",
    "\n",
    "**Result**: Epsilon decreased to 0.0174 (nearly full exploitation). Q-values approximately doubled (79000-97000 range). However, the optimal route remains identical (L9 → L8 → L7 → L4).\n",
    "\n",
    "**Observation**: For this simple 9-state environment, increasing iterations from 1000 to 5000 does not improve route quality. 1000 iterations is sufficient to find the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9yu3jae74f7",
   "metadata": {},
   "source": [
    "### Comparison: epsilon_decay = 0.01\n",
    "\n",
    "Tested with epsilon_decay = 0.01 (faster decay rate) using 1000 iterations.\n",
    "\n",
    "**Result**: Epsilon reached 0.0101 at iteration 900, nearly converged by iteration 500 (epsilon = 0.0167). Optimal route remains L9 → L8 → L7 → L4.\n",
    "\n",
    "**Advantages**:\n",
    "1. Extremely fast convergence: reaches final strategy within 500 iterations\n",
    "2. Efficient training: suitable for simple environments to quickly find solution\n",
    "3. Time-saving: 500 iterations sufficient, no need for 5000 iterations\n",
    "\n",
    "**Trade-off**: Q-table learning is uneven. Main path has high Q-values (52K-55K) but non-main paths have very low values (8-25), indicating insufficient exploration of alternative routes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vb9o3rg1jb",
   "source": "### Comparison: alpha = 0.9\n\nTested with alpha = 0.9 (higher learning rate) while keeping epsilon_decay = 0.01 and 1000 iterations.\n\n**Result**: Optimal route unchanged (L9 → L8 → L7 → L4). Main path Q-values increased to 56K-58K range (5-8% higher than alpha=0.7).\n\n**Observations**:\n\nHigher learning rate leads to faster Q-value accumulation on main path. However, non-main paths show extreme instability with values ranging from 2.6 to 27,201, indicating over-sensitivity to individual experiences.\n\n**Comparison with alpha=0.7**:\n- Main path: alpha=0.9 slightly higher and faster convergence\n- Non-main paths: alpha=0.7 more stable and reliable\n- Overall: alpha=0.7 provides better balance between learning speed and stability\n\n**Conclusion**: For this environment, alpha=0.7 is recommended as it achieves good convergence while maintaining stable Q-values across all state-action pairs.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}